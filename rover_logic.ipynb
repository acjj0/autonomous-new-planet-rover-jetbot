{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rover Logic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the logic comes together across all use-cases here. Load all models, create widgets for display, and do the entire logic for each of the use-case situations\n",
    "\n",
    "Start with importing all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from jetbot import bgr8_to_jpeg\n",
    "from jetbot import Camera\n",
    "from jetbot import Robot\n",
    "from IPython.display import display\n",
    "from jetbot import ObjectDetector\n",
    "import ipywidgets.widgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load labels from the COCO dataset into a list to serve as a lookup table for object IDs for display around bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = open(\"ms-coco-labels.txt\",'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the COCO model for object identification with 91 classes of objects. \n",
    "\n",
    "This model is used for object identification, navigation goal setting and object following. This model is not used for collision avoidance.\n",
    "\n",
    "I do not do transfer learning on this model because the rover will not know the exact types of objects on a new planet or moon. I use this model as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the collision avoidance model. This model is trained to classify objects in the camera's view into 2 classes - too near with high risk of collision vs low risk of collision. What the neural network has actually learned is amount of floor visible that's safe vs unsafe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 2)\n",
    "collision_model.load_state_dict(torch.load('best_model.pth'))\n",
    "device = torch.device('cuda')\n",
    "collision_model = collision_model.to(device)\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of images captured in a neat helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the camera, and set FPS at 5, which is sufficient for our needs and allows functioning without lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = Camera.instance(width=300, height=300, capture_width=300, capture_height=300, fps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all visual widgets of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "speed_widget = widgets.FloatSlider(value=0.2, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.2, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "objectofinterest_widget = widgets.Textarea()\n",
    "detections_widget = widgets.Textarea()\n",
    "found_widget = widgets.Label()\n",
    "\n",
    "objectofinterest_widget.value = '88' # 88 = teddy bear\n",
    "found_widget.value = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and initialize the Robot object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for object annotation. This is valuable for debugging too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_annotated_boundingbox(image, det):\n",
    "    bbox = det['bbox']\n",
    "    left = int(width * bbox[0])\n",
    "    top = int(height * bbox[1])\n",
    "    right = int(width * bbox[2])\n",
    "    bottom = int(height * bbox[3])\n",
    "    color = (255, 255, 0)\n",
    "    thickness = 2\n",
    "    confidence = \"{:.0%}\".format(det['confidence'])\n",
    "    cv2.rectangle(image, (left, top), (right, bottom), color, thickness)\n",
    "    cv2.putText(image, str(labels_list[int(det['label'])]) + \", \" + str(confidence), (left, bottom + 12), 0, 1e-3 * height, color, thickness//8)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for goal setting. Knowing the center of an object seen in an image, you can reorient the jetbot and its camera to point towards that detected object's center such that further movement of the Jetbot will go in the direction of that object's center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_boundingbox_center(detection):\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to perform the planning and motion control actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_exploring():\n",
    "    robot.forward(0.2)\n",
    "    \n",
    "def collision_avoid():\n",
    "    robot.left(0.2)\n",
    "    \n",
    "def goal_setting(matching_detection):\n",
    "    # move robot forward and steer proportional target's x-distance from center\n",
    "    center = find_boundingbox_center(matching_detection)\n",
    "    robot.set_motors(float(speed_widget.value + turn_gain_widget.value * center[0]), \n",
    "                     float(speed_widget.value - turn_gain_widget.value * center[0]))\n",
    "    \n",
    "def object_follow_reached():\n",
    "    # Some objects are detected. If the most likely one is a the object of interest\n",
    "    camera.unobserve_all()\n",
    "    robot.stop()\n",
    "    # Found an alien\n",
    "    found_widget.value = 'Found An Alien!!! Stopping here to stream alien data to Earth.'\n",
    "    robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central operation of the rover's logic, that's placed as a callback function and is called every time the Jetbot returns a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(change):\n",
    "    \n",
    "    image = change['new']\n",
    "    \n",
    "    # Perception elements\n",
    "    collision_output = collision_model(preprocess(image)).detach().cpu()\n",
    "    prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\n",
    "    blocked_widget.value = prob_blocked\n",
    "    \n",
    "    detections = model(image)\n",
    "    detections_widget.value = str(detections)\n",
    "    \n",
    "    # For all objects identified, bounding box with classification and probability\n",
    "    for det in detections[0]:   \n",
    "        draw_annotated_boundingbox(image, det)\n",
    "   \n",
    "    # Planning & Control elements\n",
    "    if prob_blocked > 0.75:\n",
    "        # We are blocked by something\n",
    "        if any(detections) and detections[0][0]['label'] == int(objectofinterest_widget.value):\n",
    "            # The blocking object matches our target, stop here to observe safely\n",
    "            object_follow_reached()\n",
    "        else:\n",
    "            # Collision probable with any further exploration in this direction\n",
    "            collision_avoid()\n",
    "    else:\n",
    "        # Not blocked by anything, so see if any object in our view matches our target\n",
    "        matching_detections = [d for d in detections[0] if d['label'] == int(objectofinterest_widget.value)]\n",
    "        \n",
    "        if len(matching_detections) > 0:\n",
    "            # One or more objects in the image matches our target\n",
    "            goal_setting(matching_detections[0])\n",
    "        else:\n",
    "            # No object detected in the image matches our target\n",
    "            keep_exploring()\n",
    "    \n",
    "    cv2.resize(image,(600, 600))\n",
    "    image_widget.value = bgr8_to_jpeg(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display all UI widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed036e27eda46f1ae602d7c1736d6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='300', width='300')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d089f8ca66f4322941ce340a8fa2b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='blocked', max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fc5efd8bff4054802bdf60d73e895e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.2, description='speed', max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778942d2f6924c67bea542307e36b4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.2, description='turn gain', max=2.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671f403db6f8431c84136af209dae1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2541d99e5a4d0e89ec29fc91ce04db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22e85a781bc4610861730c78d02f573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='88')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(image_widget)\n",
    "display(blocked_widget)\n",
    "display(speed_widget)\n",
    "display(turn_gain_widget)\n",
    "display(detections_widget)\n",
    "display(found_widget)\n",
    "display(objectofinterest_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close any existing handles to the camera and create a fresh new one which runs the callback function whenever the camera sends in the next image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the camera and the robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "camera.unobserve_all()\n",
    "robot.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
